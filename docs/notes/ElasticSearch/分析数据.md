# 分析数据

- 分析器分析流程【分析是通过文档字段的文本，生成分词的过程】
  字符过滤---分词---分词过滤[转小写、删除停用词、添加同义词]---分词索引
- 分词 api----\_analyze
  ```js
  GET _analyze
  {
    "tokenizer": "whitespace",
    "filter": ["lowercase", "reverse"],
    "text": ["Good Person & big data"]
  }
  ```
- 1、分析器
  - standard analyzer 是文本的默认分析器
  - simple analyzer 简单分析器（只使用了小写转换分词器，在非字母处进行分词，适用于欧洲语言）
  - whitespace analyzer 空白分析器（根据空白对文档分词）
  - stop analyzer 停用词分析器（类似 simple 分析器，另加了停用词进行分词）
  - keyword analyzer 关键词分析器（最好将 index 设置指定位 not_analyzed,而不是在映射中使用关键词分析器）
  - pattern analyzer 模式分析器（允许指定一个分词切分的模式）
  - 多语言分析器
  - snowball 雪球分析器（standard+小写分词过滤器+停用词过滤器+雪球词干器）
- 2、分词器
  - standard tokenizer 标准分词器（默认最大长度为 255，移除标点符号）
  - keyword tokenizer 关键词分词器（将整个文本作为单个分词，移交给分词过滤器）
  - pattern tokenizer 模式分词器
  ```js
  GET _analyze
  {
    "tokenizer": {
      "type": "pattern",
      "pattern": "\\."
    },
    "filter": ["lowercase", "reverse"],
    "text": ["Good Person & big data. I have, potatoes"]
  }
  ```
  - UAX URL 电子邮件分词器
  - path hierarchy tokenizer 路径层次分词器
    ......
- 3、分词过滤器[接收从分词器出来的分词，然后为索引准备数据]-如：转小写、停用词、添加同义词

  - standard token filter 标准分词过滤器（什么都没做）
  - lowercase token filter 小写分词过滤器
  - length token filter 长度分词过滤器
  - stop token filter 停用词分词过滤器（默认停用词：a,an,and,are,as,at,be,but...）
  - reverse token filter 颠倒分词过滤器（适用于前通配搜索、侧边 N 元语法过滤器？）
  - unique token filter 唯一分词过滤器（去除重复的分词）
  - synonym token filter 同义词分词过滤器
    ......

- 4、N 元语法（ngram）、侧边 N 元语法（edge ngram）、滑动窗口（shingles）
  当使用 N 元语法分析器的时候，需要设置 min_gram、max_gram
  优势：拼错单词，也可以找到对应的文档
  eg. spaghetti ---trigrams[三元语法]---spa、pag、agh、ghe...
  eg. spaghetti ---[侧边 N 元语法过滤器]（min_gram=2,max_gram=6）---sp、spa、spag、spagh、spaghe
  eg. foo bar baz ---[滑动窗口分词过滤器]（min_shingles_size=2,max_shingles_size=3）--- foo bar、bar baz、foo bar baz
  【shingles 默认包含了原始分词。output_unigrams：false，禁用原始分词】
  ```js
  GET _analyze
  {
    "tokenizer": "standard",
    "filter": [
      {
        "type": "shingle",
        "min_shingle_size": 2,
        "max_shingle_size": 3,
        "output_unigrams": false
      }
    ],
    "text": [
      "foo bar baz"
    ]
  }
  ```
- 5、提取词干
  如：administrations -> 提取词干 administr -> 使用 administr 可匹配“administration、administrator、administrate...”
  三种不同的词干算法：snowball 过滤器、porter_stem 过滤器、kstem 过滤器

  ```js
  GET _analyze
  {
    "tokenizer": "standard",
    "filter": [
      "porter_stem"
    ],
    "text": [
      "administrators"
    ]
  }
  ```

  - 使用字典提取词干（认为 ES 提供的算法词干提取很奇怪的话，精确）
  - 重写分词过滤器的词干提取（有时候不想提取单词词干）

- 6、注意点
  - match 查询的搜索字符串，同样会分词。用分词去匹配文档的分词。
  - 通过映射，每个字段都会分配一个分析器。
  - 分析器：字符过滤器--**分词器**--分词过滤器
